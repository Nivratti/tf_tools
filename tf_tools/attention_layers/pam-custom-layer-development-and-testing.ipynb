{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <font color=\"#000080\" size=\"+3\">1. Setup</font>\n","metadata":{}},{"cell_type":"code","source":"!pip show tensorflow","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:14:24.409774Z","iopub.execute_input":"2024-04-27T11:14:24.410244Z","iopub.status.idle":"2024-04-27T11:14:39.470857Z","shell.execute_reply.started":"2024-04-27T11:14:24.410201Z","shell.execute_reply":"2024-04-27T11:14:39.469219Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Name: tensorflow\nVersion: 2.15.0\nSummary: TensorFlow is an open source machine learning framework for everyone.\nHome-page: https://www.tensorflow.org/\nAuthor: Google Inc.\nAuthor-email: packages@tensorflow.org\nLicense: Apache 2.0\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\nRequired-by: explainable-ai-sdk, tensorflow-cloud, tensorflow-decision-forests, tensorflow-serving-api, tensorflow-text, tf_keras, witwidget\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip show keras","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:15:56.801698Z","iopub.execute_input":"2024-04-27T11:15:56.802248Z","iopub.status.idle":"2024-04-27T11:16:11.463609Z","shell.execute_reply.started":"2024-04-27T11:15:56.802201Z","shell.execute_reply":"2024-04-27T11:16:11.461888Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"Name: keras\nVersion: 3.3.3\nSummary: Multi-backend Keras.\nHome-page: https://github.com/keras-team/keras\nAuthor: Keras team\nAuthor-email: keras-users@googlegroups.com\nLicense: Apache License 2.0\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: absl-py, h5py, ml-dtypes, namex, numpy, optree, rich\nRequired-by: keras-tuner, tensorflow\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --quiet keras==3.3.3\n!pip show keras","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:15:34.748174Z","iopub.execute_input":"2024-04-27T11:15:34.748645Z","iopub.status.idle":"2024-04-27T11:15:50.815163Z","shell.execute_reply.started":"2024-04-27T11:15:34.748605Z","shell.execute_reply":"2024-04-27T11:15:50.813335Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"#000080\" size=\"+3\">2. Custom layer</font>\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, backend as K\n\nclass PAM(layers.Layer):\n    \"\"\"\n    Position Attention Module (PAM) as a custom Keras Layer.\n\n    This layer captures spatial attention mechanisms within a feature map.\n    It uses three convolutional layers to project the input into different\n    spaces to calculate attention, then scales the input feature map by the\n    attention scores to enhance features with inter-spatial relevance.\n\n    Attributes:\n        gamma_initializer (str or keras.initializers): Initializer for the scaling factor.\n        gamma_regularizer (str or keras.regularizers): Regularizer for the scaling factor.\n        gamma_constraint (str or keras.constraints): Constraint for the scaling factor.\n        activation (str): The type of activation function to use ('softmax' or 'sigmoid').\n    \"\"\"\n    def __init__(self, gamma_initializer='zeros', gamma_regularizer=None, gamma_constraint=None, activation='sigmoid', **kwargs):\n        super(PAM, self).__init__(**kwargs)\n        self.gamma_initializer = gamma_initializer\n        self.gamma_regularizer = gamma_regularizer\n        self.gamma_constraint = gamma_constraint\n        self.activation = activation\n\n    def build(self, input_shape):\n        if input_shape[-1] is None:\n            raise ValueError(\"The channel dimension of the inputs should be defined. Found `None`.\")\n\n        self.gamma = self.add_weight(\n            name='gamma', \n            shape=(1,),\n            initializer=self.gamma_initializer,\n            regularizer=self.gamma_regularizer,\n            constraint=self.gamma_constraint\n        )\n\n        num_channels = input_shape[-1]\n        self.conv_b = layers.Conv2D(num_channels // 8, 1, use_bias=False, kernel_initializer='he_normal')\n        self.conv_c = layers.Conv2D(num_channels // 8, 1, use_bias=False, kernel_initializer='he_normal')\n        self.conv_d = layers.Conv2D(num_channels, 1, use_bias=False, kernel_initializer='he_normal')\n    \n        # Build the internal Conv2D layers with the correct input shape\n        self.conv_b.build(input_shape)\n        self.conv_c.build(input_shape)\n        self.conv_d.build(input_shape)\n        \n        super(PAM, self).build(input_shape)\n\n    def call(self, inputs):\n        shape = tf.shape(inputs)\n        batch_size, h, w, filters = shape[0], shape[1], shape[2], shape[3]\n\n\n        # Convolution layers to transform the input into two feature spaces B and C\n        b = self.conv_b(inputs)\n        c = self.conv_c(inputs)\n        # Convolution layer to transform the input for output scaling\n        d = self.conv_d(inputs)\n\n        # Reshape B and C for matrix multiplication\n        vec_b = K.reshape(b, (batch_size, h * w, filters // 8))\n        vec_cT = K.permute_dimensions(K.reshape(c, (batch_size, h * w, filters // 8)), (0, 2, 1))\n\n        # Attention map created by matrix multiplying B and C^T\n        bcT = K.batch_dot(vec_b, vec_cT)\n        \n        # Apply activation to attention scores\n        if self.activation == 'softmax':\n            attention_scores = layers.Softmax(axis=-1)(bcT)\n        elif self.activation == 'sigmoid':\n            attention_scores = layers.Activation('sigmoid')(bcT)\n        else:\n            raise ValueError(\"Unsupported activation function. Choose 'softmax' or 'sigmoid'.\")\n\n        # Scale output with attention scores\n        vec_d = K.reshape(d, (batch_size, h * w, filters))\n        bcTd = K.batch_dot(attention_scores, vec_d)\n        bcTd = K.reshape(bcTd, (batch_size, h, w, filters))\n\n        # Apply the learnable parameter gamma to scale the attended output\n        out = self.gamma * bcTd + inputs\n        return out\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:28:53.837808Z","iopub.execute_input":"2024-04-27T11:28:53.838792Z","iopub.status.idle":"2024-04-27T11:28:53.860613Z","shell.execute_reply.started":"2024-04-27T11:28:53.838740Z","shell.execute_reply":"2024-04-27T11:28:53.858976Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input\nimport keras\n\n# Define the input tensor\ninput_tensor = Input(shape=(None, None, 1408))  # Replace 'channels' with the actual number\n\n# Create an instance of your custom layer\nattention_layer = PAM()\n\n# Apply your custom layer\noutput_tensor = attention_layer(input_tensor)\n\n# Build the model\nmodel = Model(inputs=input_tensor, outputs=output_tensor)\n\n# Summary of the model to see all parameters including those from PAM\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:28:20.731902Z","iopub.execute_input":"2024-04-27T11:28:20.732526Z","iopub.status.idle":"2024-04-27T11:28:20.842179Z","shell.execute_reply.started":"2024-04-27T11:28:20.732481Z","shell.execute_reply":"2024-04-27T11:28:20.840435Z"},"trusted":true},"execution_count":81,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_60\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_60\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_27 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │             \u001b[38;5;34m0\u001b[0m │\n│                                 │ \u001b[38;5;34m1408\u001b[0m)                  │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pam_44 (\u001b[38;5;33mPAM\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │     \u001b[38;5;34m2,478,081\u001b[0m │\n│                                 │ \u001b[38;5;34m1408\u001b[0m)                  │               │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)                  │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pam_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PAM</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,478,081</span> │\n│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)                  │               │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,478,081\u001b[0m (9.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,478,081</span> (9.45 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,478,081\u001b[0m (9.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,478,081</span> (9.45 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"# Total params: 2,478,081 (9.45 MB)\n#  Trainable params: 2,478,081 (9.45 MB)\n#  Non-trainable params: 0 (0.00 B)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"#000080\" size=\"+3\">3. Unit testing</font>\n","metadata":{}},{"cell_type":"markdown","source":"### <font color=\"amber\" size=\"+2\">3.1 Shape testing</font>","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras import layers\n\ndef test_pam_shape(input_shape):\n    \"\"\"\n    Tests whether the PAM layer retains the same input and output shape.\n    \n    Args:\n    input_shape (tuple): The shape of the input tensor to test, excluding batch size.\n    \n    Returns:\n    bool: True if the shape test passes, False otherwise.\n    \"\"\"\n    # Create a random tensor with the specified shape\n    input_tensor = tf.random.normal([1] + list(input_shape))\n\n    # Initialize the PAM layer\n    pam_layer = PAM()\n\n    # Get the output from the PAM layer\n    output_tensor = pam_layer(input_tensor)\n\n    # Check if the output shape matches the input shape\n    if input_tensor.shape == output_tensor.shape:\n        print(\"Shape Test Passed: Input shape and output shape are the same.\")\n        return True\n    else:\n        print(\"Shape Test Failed: Output shape does not match input shape.\")\n        return False\n\n# Example usage\ntest_pam_shape((128, 128, 32))","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:28:57.720388Z","iopub.execute_input":"2024-04-27T11:28:57.720886Z","iopub.status.idle":"2024-04-27T11:28:59.179598Z","shell.execute_reply.started":"2024-04-27T11:28:57.720844Z","shell.execute_reply":"2024-04-27T11:28:59.178034Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"Shape Test Passed: Input shape and output shape are the same.\n","output_type":"stream"},{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"### <font color=\"amber\" size=\"+2\">3.2 Functionality testing</font>","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ndef create_patterned_input(input_shape, pattern_size=(2, 2)):\n    \"\"\"\n    Creates a test input tensor with zeros and a specific pattern in the center.\n    \n    Args:\n    input_shape (tuple): The shape of the input tensor, excluding batch size.\n    pattern_size (tuple): The size of the square pattern to be placed in the center.\n    \n    Returns:\n    tf.Tensor: The created input tensor with a pattern.\n    \"\"\"\n    # Full zeros tensor\n    base_input = tf.zeros([1] + list(input_shape), dtype=tf.float32)\n    \n    # Calculate the start indices for the pattern\n    start_idx_h = input_shape[0] // 2 - pattern_size[0] // 2\n    start_idx_w = input_shape[1] // 2 - pattern_size[1] // 2\n\n    # Create indices for updates\n    indices = []\n    updates = []\n    for i in range(pattern_size[0]):\n        for j in range(pattern_size[1]):\n            for k in range(input_shape[2]):\n                indices.append([0, start_idx_h + i, start_idx_w + j, k])\n                updates.append(1.0)\n\n    # Convert lists to tensors\n    indices = tf.constant(indices, dtype=tf.int32)\n    updates = tf.constant(updates, dtype=tf.float32)\n\n    # Update the base input tensor with a pattern\n    pattern_input = tf.tensor_scatter_nd_update(base_input, indices, updates)\n    return pattern_input\n\ndef test_pam_functionality(input_shape):\n    \"\"\"\n    Tests the functionality of the PAM layer to ensure it alters the input tensor.\n    \n    Args:\n    input_shape (tuple): The shape of the input tensor to test, excluding batch size.\n    \n    Returns:\n    bool: True if the functionality test passes, False otherwise.\n    \"\"\"\n    # Create a controlled test input with a specific pattern\n    test_input = create_patterned_input(input_shape, pattern_size=(1, 1))\n\n    # Initialize the PAM layer\n    pam_layer = PAM(gamma_initializer='ones')\n\n    # Get the output from the PAM layer\n    output_tensor = pam_layer(test_input)\n\n    # Calculate mean of the input and output tensors\n    input_mean = tf.reduce_mean(test_input)\n    output_mean = tf.reduce_mean(output_tensor)\n\n    # Check if there is a statistical difference between the input and output\n    if not tf.math.equal(input_mean, output_mean):\n        print(\"Functionality Test Passed: Output is statistically different from the input.\")\n        return True\n    else:\n        print(\"Functionality Test Failed: Output is statistically the same as the input.\")\n        return False\n\n# Example usage\ntest_pam_functionality((2, 20, 1408))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:29:04.036667Z","iopub.execute_input":"2024-04-27T11:29:04.037396Z","iopub.status.idle":"2024-04-27T11:29:04.146825Z","shell.execute_reply.started":"2024-04-27T11:29:04.037359Z","shell.execute_reply":"2024-04-27T11:29:04.145020Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"Functionality Test Passed: Output is statistically different from the input.\n","output_type":"stream"},{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]}]}