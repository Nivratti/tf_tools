{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <font color=\"#000080\" size=\"+3\">1. Setup</font>\n","metadata":{}},{"cell_type":"code","source":"!pip show tensorflow","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:14:24.409774Z","iopub.execute_input":"2024-04-27T11:14:24.410244Z","iopub.status.idle":"2024-04-27T11:14:39.470857Z","shell.execute_reply.started":"2024-04-27T11:14:24.410201Z","shell.execute_reply":"2024-04-27T11:14:39.469219Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Name: tensorflow\nVersion: 2.15.0\nSummary: TensorFlow is an open source machine learning framework for everyone.\nHome-page: https://www.tensorflow.org/\nAuthor: Google Inc.\nAuthor-email: packages@tensorflow.org\nLicense: Apache 2.0\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\nRequired-by: explainable-ai-sdk, tensorflow-cloud, tensorflow-decision-forests, tensorflow-serving-api, tensorflow-text, tf_keras, witwidget\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip show keras","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:15:56.801698Z","iopub.execute_input":"2024-04-27T11:15:56.802248Z","iopub.status.idle":"2024-04-27T11:16:11.463609Z","shell.execute_reply.started":"2024-04-27T11:15:56.802201Z","shell.execute_reply":"2024-04-27T11:16:11.461888Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"Name: keras\nVersion: 3.3.3\nSummary: Multi-backend Keras.\nHome-page: https://github.com/keras-team/keras\nAuthor: Keras team\nAuthor-email: keras-users@googlegroups.com\nLicense: Apache License 2.0\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: absl-py, h5py, ml-dtypes, namex, numpy, optree, rich\nRequired-by: keras-tuner, tensorflow\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install --quiet keras==3.3.3\n# !pip show keras","metadata":{"execution":{"iopub.status.busy":"2024-04-27T11:15:34.748174Z","iopub.execute_input":"2024-04-27T11:15:34.748645Z","iopub.status.idle":"2024-04-27T11:15:50.815163Z","shell.execute_reply.started":"2024-04-27T11:15:34.748605Z","shell.execute_reply":"2024-04-27T11:15:50.813335Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"#000080\" size=\"+3\">2. Custom layer</font>\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras import layers\nfrom tensorflow.keras import backend as K\n\nclass PAM(layers.Layer):\n    \"\"\"\n    Position Attention Module (PAM) as a custom Keras Layer.\n\n    Captures spatial attention mechanisms within a feature map by projecting the input\n    into different spaces to calculate attention, then scales the input feature map by\n    the attention scores to enhance features with inter-spatial relevance.\n\n    Attributes:\n        scale_gamma_initializer: Initializer for the scaling factor.\n        scale_gamma_regularizer: Regularizer for the scaling factor.\n        scale_gamma_constraint: Constraint for the scaling factor.\n        activation_func: The type of activation function to use ('softmax' or 'sigmoid').\n    \"\"\"\n    def __init__(self, scale_gamma_initializer='zeros', scale_gamma_regularizer=None, scale_gamma_constraint=None, activation_func='sigmoid', **kwargs):\n        super(PAM, self).__init__(**kwargs)\n        self.scale_gamma_initializer = scale_gamma_initializer\n        self.scale_gamma_regularizer = scale_gamma_regularizer\n        self.scale_gamma_constraint = scale_gamma_constraint\n        self.activation_func = activation_func\n\n    def build(self, input_shape):\n        if input_shape[-1] is None:\n            raise ValueError(\"The channel dimension of the inputs should be defined. Found `None`.\")\n\n        self.scale_gamma = self.add_weight(\n            name='scale_gamma', \n            shape=(1,),\n            initializer=self.scale_gamma_initializer,\n            regularizer=self.scale_gamma_regularizer,\n            constraint=self.scale_gamma_constraint\n        )\n\n        num_channels = input_shape[-1]\n        self.query_conv = layers.Conv2D(num_channels // 8, 1, use_bias=False, kernel_initializer='he_normal')\n        self.key_conv = layers.Conv2D(num_channels // 8, 1, use_bias=False, kernel_initializer='he_normal')\n        self.value_conv = layers.Conv2D(num_channels, 1, use_bias=False, kernel_initializer='he_normal')\n\n        # Build the internal Conv2D layers with the correct input shape\n        self.query_conv.build(input_shape)\n        self.key_conv.build(input_shape)\n        self.value_conv.build(input_shape)\n        super(PAM, self).build(input_shape)\n\n    def call(self, inputs):\n        # Extract dimensions to handle dynamic shape scenarios\n        shape = tf.shape(inputs)\n        batch_size, height, width, num_filters = shape[0], shape[1], shape[2], shape[3]\n\n        # Generate query and key features\n        query_features = self.query_conv(inputs)\n        key_features = self.key_conv(inputs)\n\n        # Prepare for matrix multiplication by reshaping query and key features\n        query_flat = K.reshape(query_features, (batch_size, height * width, num_filters // 8))\n        key_flat_transposed = K.permute_dimensions(K.reshape(key_features, (batch_size, height * width, num_filters // 8)), (0, 2, 1))\n\n        # Compute attention scores using matrix multiplication\n        attention_scores = K.batch_dot(query_flat, key_flat_transposed)\n        \n        # Apply the specified activation function to the attention scores\n        if self.activation_func == 'softmax':\n            attention_scores = layers.Softmax(axis=-1)(attention_scores)\n        elif self.activation_func == 'sigmoid':\n            attention_scores = layers.Activation('sigmoid')(attention_scores)\n        else:\n            raise ValueError(f\"Unsupported activation function '{self.activation_func}'. Choose 'softmax' or 'sigmoid'.\")\n\n        # Compute output features by applying the attention scores to the value features\n        value_features = self.value_conv(inputs) # Convolution layer to transform the input for output scaling\n        value_flat = K.reshape(value_features, (batch_size, height * width, num_filters))\n        attended_features = K.batch_dot(attention_scores, value_flat)\n        attended_features_reshaped = K.reshape(attended_features, (batch_size, height, width, num_filters))\n\n        # Scale the output by the learned gamma factor and add the input\n        output = self.scale_gamma * attended_features_reshaped + inputs\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:57:40.532315Z","iopub.execute_input":"2024-04-27T14:57:40.532797Z","iopub.status.idle":"2024-04-27T14:57:57.915915Z","shell.execute_reply.started":"2024-04-27T14:57:40.532762Z","shell.execute_reply":"2024-04-27T14:57:57.914731Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-04-27 14:57:42.950345: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-27 14:57:42.950559: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-27 14:57:43.120199: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input\nimport keras\n\n# Define the input tensor\ninput_tensor = Input(shape=(None, None, 1408))  # Replace 'channels' with the actual number\n\n# Create an instance of your custom layer\nattention_layer = PAM()\n\n# Apply your custom layer\noutput_tensor = attention_layer(input_tensor)\n\n# Build the model\nmodel = Model(inputs=input_tensor, outputs=output_tensor)\n\n# Summary of the model to see all parameters including those from PAM\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T13:32:55.682954Z","iopub.execute_input":"2024-04-27T13:32:55.683822Z","iopub.status.idle":"2024-04-27T13:32:55.771276Z","shell.execute_reply.started":"2024-04-27T13:32:55.683786Z","shell.execute_reply":"2024-04-27T13:32:55.770532Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_7\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │             \u001b[38;5;34m0\u001b[0m │\n│                                 │ \u001b[38;5;34m1408\u001b[0m)                  │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pam (\u001b[38;5;33mPAM\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │     \u001b[38;5;34m2,478,081\u001b[0m │\n│                                 │ \u001b[38;5;34m1408\u001b[0m)                  │               │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)                  │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pam (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PAM</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,478,081</span> │\n│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)                  │               │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,478,081\u001b[0m (9.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,478,081</span> (9.45 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,478,081\u001b[0m (9.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,478,081</span> (9.45 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"# Total params: 2,478,081 (9.45 MB)\n#  Trainable params: 2,478,081 (9.45 MB)\n#  Non-trainable params: 0 (0.00 B)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=\"#000080\" size=\"+3\">3. Unit testing</font>\n","metadata":{}},{"cell_type":"markdown","source":"### <font color=\"amber\" size=\"+2\">3.1 Shape testing</font>","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras import layers\n\ndef test_pam_shape(input_shape):\n    \"\"\"\n    Tests whether the PAM layer retains the same input and output shape.\n    \n    Args:\n    input_shape (tuple): The shape of the input tensor to test, excluding batch size.\n    \n    Returns:\n    bool: True if the shape test passes, False otherwise.\n    \"\"\"\n    # Create a random tensor with the specified shape\n    input_tensor = tf.random.normal([1] + list(input_shape))\n\n    # Initialize the PAM layer\n    pam_layer = PAM()\n\n    # Get the output from the PAM layer\n    output_tensor = pam_layer(input_tensor)\n\n    # Check if the output shape matches the input shape\n    if input_tensor.shape == output_tensor.shape:\n        print(\"Shape Test Passed: Input shape and output shape are the same.\")\n        return True\n    else:\n        print(\"Shape Test Failed: Output shape does not match input shape.\")\n        return False\n\n# Example usage\ntest_pam_shape((128, 128, 32))","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:00:48.002714Z","iopub.execute_input":"2024-04-27T15:00:48.003894Z","iopub.status.idle":"2024-04-27T15:00:50.167198Z","shell.execute_reply.started":"2024-04-27T15:00:48.003824Z","shell.execute_reply":"2024-04-27T15:00:50.165789Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Shape Test Passed: Input shape and output shape are the same.\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"### <font color=\"amber\" size=\"+2\">3.2 Functionality testing</font>","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ndef create_patterned_input(input_shape, pattern_size=(2, 2)):\n    \"\"\"\n    Creates a test input tensor with zeros and a specific pattern in the center.\n    \n    Args:\n    input_shape (tuple): The shape of the input tensor, excluding batch size.\n    pattern_size (tuple): The size of the square pattern to be placed in the center.\n    \n    Returns:\n    tf.Tensor: The created input tensor with a pattern.\n    \"\"\"\n    # Full zeros tensor\n    base_input = tf.zeros([1] + list(input_shape), dtype=tf.float32)\n    \n    # Calculate the start indices for the pattern\n    start_idx_h = input_shape[0] // 2 - pattern_size[0] // 2\n    start_idx_w = input_shape[1] // 2 - pattern_size[1] // 2\n\n    # Create indices for updates\n    indices = []\n    updates = []\n    for i in range(pattern_size[0]):\n        for j in range(pattern_size[1]):\n            for k in range(input_shape[2]):\n                indices.append([0, start_idx_h + i, start_idx_w + j, k])\n                updates.append(1.0)\n\n    # Convert lists to tensors\n    indices = tf.constant(indices, dtype=tf.int32)\n    updates = tf.constant(updates, dtype=tf.float32)\n\n    # Update the base input tensor with a pattern\n    pattern_input = tf.tensor_scatter_nd_update(base_input, indices, updates)\n    return pattern_input\n\ndef test_pam_functionality(input_shape):\n    \"\"\"\n    Tests the functionality of the PAM layer to ensure it alters the input tensor.\n    \n    Args:\n    input_shape (tuple): The shape of the input tensor to test, excluding batch size.\n    \n    Returns:\n    bool: True if the functionality test passes, False otherwise.\n    \"\"\"\n    # Create a controlled test input with a specific pattern\n    test_input = create_patterned_input(input_shape, pattern_size=(1, 1))\n\n    # Initialize the PAM layer\n    pam_layer = PAM(scale_gamma_initializer='ones')\n\n    # Get the output from the PAM layer\n    output_tensor = pam_layer(test_input)\n\n    # Calculate mean of the input and output tensors\n    input_mean = tf.reduce_mean(test_input)\n    output_mean = tf.reduce_mean(output_tensor)\n\n    # Check if there is a statistical difference between the input and output\n    if not tf.math.equal(input_mean, output_mean):\n        print(\"Functionality Test Passed: Output is statistically different from the input.\")\n        return True\n    else:\n        print(\"Functionality Test Failed: Output is statistically the same as the input.\")\n        return False\n\n# Example usage\ntest_pam_functionality((2, 20, 1408))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:00:45.277804Z","iopub.execute_input":"2024-04-27T15:00:45.278315Z","iopub.status.idle":"2024-04-27T15:00:45.508307Z","shell.execute_reply.started":"2024-04-27T15:00:45.278275Z","shell.execute_reply":"2024-04-27T15:00:45.506988Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Functionality Test Passed: Output is statistically different from the input.\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# if __name__ == \"__main__\":\n#     from keras.models import Model\n#     from keras.layers import Input\n#     import keras\n\n#     # Define the input tensor\n#     # input_tensor = Input(shape=(64, 640, 1408))\n#     input_tensor = Input(shape=(None, None, 1408))\n\n#     # Create an instance of your custom layer\n#     attention_layer = PAM()\n\n#     # Apply your custom layer\n#     output_tensor = attention_layer(input_tensor)\n\n#     # Build the model\n#     model = Model(inputs=input_tensor, outputs=output_tensor)\n\n#     # Summary of the model to see all parameters including those from PAM\n#     print(model.summary())\n    \n#     print(f\"\\nPerforming unit testing:\")\n#     ## 2. Unit testing\n#     ### 2.1 shape testing\n#     test_pam_shape((128, 128, 32))\n    \n#     ### 2.2 Functionality testing\n#     test_pam_functionality((2, 20, 1408))","metadata":{},"execution_count":null,"outputs":[]}]}